import tempfile
import logging
import os
import subprocess
import docker
import random
import string
import time
import httpx
from unittest.mock import MagicMock
from anthropic import AnthropicBedrock
from anthropic.types import Message, TextBlock, Usage, ToolUseBlock
from application import Application, langfuse_context, feature_flags
from compiler.core import Compiler
from core.interpolator import Interpolator

logging.basicConfig(level=logging.INFO)


def _wrap_anthropic_response(text: str | None = None, tool_use: dict | None = None):
    if text is not None:
        content = [TextBlock(type="text", text=text)]
    else:
        content = [ToolUseBlock(id="tool_use", input=tool_use, name="extract_user_functions", type="tool_use")]

    return Message(
        id="msg_123",
        type="message",
        role="assistant",
        content=content,
        model="claude-3-5-sonnet-20241022",
        usage=Usage(
            input_tokens=10,
            output_tokens=20
        )
    )


def get_real_antropic_response(prompt: str | None = None, messages: list[dict] | None = None):
    if messages is None:
        messages = [{"role": "user", "content": prompt or ""}]
    client = AnthropicBedrock(aws_profile="dev", aws_region="us-west-2")
    response = client.messages.create(messages=messages,
        max_tokens=8192,
        model="anthropic.claude-3-5-sonnet-20241022-v2:0"

    )
    return response.content[-1].text


def _get_pseudo_llm_response(*args, **kwargs):
    messages = kwargs["messages"]
    prompt = messages[-1]["content"]
    text = None
    tool_use = None

    if "generate TypeSpec models" in prompt:
        print("\tLLM: generate TypeSpec models")
        text = """
        <reasoning>
        Based on the description, I'll create a simple response bot with following considerations:
        1. Need a single function to handle inputs, etc
        </reasoning>

        <typespec>
        model InputMessage {
            content: string;
        }

        model ResponseMessage {
            reply: string;
        }

        interface SimpleResponseBot {
            @scenario(\"\"\"
            Given a user input, the bot should generate a response
            When the user input is a string, the bot should generate a response
            When the user input is a number, the bot should generate a response
            When the user input is a mixed input, the bot should generate a response
            \"\"\")
        @llm_func("process user input and generate response")
            processInput(options: InputMessage): ResponseMessage;
        }
        </typespec>
        """
    elif "Refine the following prompt" in prompt:
        print("\tLLM: refine prompt")
        text = """
        <reasoning>
        This is a simple chatbot with very basic logic...
        </reasoning>

        <requirements>
        A simple response bot that follows given rules...
        </requirements>
        """

    elif "generate Zod TypeScript data types" in prompt:
        print("\tLLM: generate TS data types")
        text = """
        <reasoning>
        The application is a simple bot that processes input messages and generates responses.
        </reasoning>

        <typescript>
        import { z } from 'zod';

        export const inputMessageSchema = z.object({
            content: z.string(),
        });

        export type InputMessage = z.infer<typeof inputMessageSchema>;

        export const responseMessageSchema = z.object({
            reply: z.string(),
        });

        export type ResponseMessage = z.infer<typeof responseMessageSchema>;

        export declare function processInput(options: InputMessage): Promise<ResponseMessage>;
        </typescript>
        """
    elif "generate Drizzle schema" in prompt:
        print("\tLLM: generate Drizzle schema")
        text = """
        <reasoning>
        hello world
        </reasoning>

        <drizzle>
        import { integer, pgTable, text, timestamp, boolean } from "drizzle-orm/pg-core";

        export const messagesTable = pgTable("messages", {
          id: integer().primaryKey().generatedAlwaysAsIdentity(),
          content: text().notNull(),
          is_response: boolean().default(false).notNull(),
          created_at: timestamp().defaultNow().notNull(),
        });

        export const conversationsTable = pgTable("conversations", {
          id: integer().primaryKey().generatedAlwaysAsIdentity(),
          created_at: timestamp().defaultNow().notNull(),
        });

        export const conversationMessagesTable = pgTable("conversation_messages", {
          id: integer().primaryKey().generatedAlwaysAsIdentity(),
          conversation_id: integer()
            .references(() => conversationsTable.id)
            .notNull(),
          message_id: integer()
            .references(() => messagesTable.id)
            .notNull(),
          sequence_number: integer().notNull(),
          created_at: timestamp().defaultNow().notNull(),
        });
        </drizzle>
        """
    elif "generate prompt for the LLM to classify which function should handle user request" in prompt:
        print("\tLLM: generate prompt for the LLM to classify which function should handle user request")
        tool_use = {
          "user_functions":
              [
                  {
                      "name": "processInput",
                      "description": "Process user input messages and generate appropriate responses...",
                      "examples": [
                          "Hello, how are you?",
                          "What's the weather like?",
                          "Can you help me with something?",
                          "Tell me a joke",
                          "What can you do?",
                          "What is your name?"
                      ]
                  }
              ]
        }

    elif "generate a unit test suite" in prompt:
        print("\tLLM: generate a unit test suite for")
        text = """
        I'll help generate unit tests for the processInput function based on the provided schemas. Here's the test suite:

        <imports>
        import { expect, it } from "bun:test";
        import { db } from "../../db";
        import { messagesTable} from "../../db/schema/application";
        import { eq } from "drizzle-orm";
        import type { InputMessage } from "../../common/schema";
        </imports>

        <test>
        it("should process input and return response", async () => {
          const input: InputMessage = { content: "Hello AI" };
          const response = await processInput(input);
          expect(response).toBeDefined();
          expect(response.reply).toBeDefined();
          expect(typeof response.reply).toBe("string");
        });
        </test>

        <test>
        it("should store input message in database", async () => {
          const input: InputMessage = { content: "Test message" };
          await processInput(input);

          const messages = await db
            .select()
            .from(messagesTable)
            .where(eq(messagesTable.is_response, false))
            .execute();

          expect(messages).toHaveLength(1);
          expect(messages[0].content).toBe("Test message");
        });
        </test>
        """

    elif "generate a handler" in prompt:
        print("\tLLM: generate a handler")
        text = """
        I'll help you create a handler for the processInput function that follows the given requirements and style guidelines.

        <handler>
        import { type InputMessage } from "../common/schema";
        import { processInput } from "../common/schema";
        import { db } from "../db";
        import { messagesTable, conversationsTable, conversationMessagesTable } from "../db/schema/application";

        export const handle: typeof processInput = async (options: InputMessage): Promise<{ reply: string }> => {
            // Start a transaction to ensure data consistency
            return await db.transaction(async (tx) => {
                // Create a new conversation
                const [conversation] = await tx
                    .insert(conversationsTable)
                    .values({})
                    .returning({ id: conversationsTable.id });

                // Store the input message
                const [inputMessage] = await tx
                    .insert(messagesTable)
                    .values({
                        content: options.content,
                        is_response: false,
                    })
                    .returning({ id: messagesTable.id });

                // Link input message to conversation
                await tx
                    .insert(conversationMessagesTable)
                    .values({
                        conversation_id: conversation.id,
                        message_id: inputMessage.id,
                        sequence_number: 1,
                    });

                // Generate response
                const responseText = `Received: ${options.content}`;

                // Store the response message
                const [responseMessage] = await tx
                    .insert(messagesTable)
                    .values({
                        content: responseText,
                        is_response: true,
                    })
                    .returning({ id: messagesTable.id });

                // Link response message to conversation
                await tx
                    .insert(conversationMessagesTable)
                    .values({
                        conversation_id: conversation.id,
                        message_id: responseMessage.id,
                        sequence_number: 2,
                    });

                return {
                    reply: responseText,
                };
            });
        };
        </handler>
        """

    else:
        raise ValueError(f"Unrecognized prompt: {prompt}")

    return _wrap_anthropic_response(text=text, tool_use=tool_use)

def _anthropic_client(text: str):
    client = MagicMock(spec=AnthropicBedrock)
    client.messages = MagicMock()
    client.messages.create = MagicMock(wraps=_get_pseudo_llm_response)
    return client


def test_end2end():
    compiler = Compiler("botbuild/tsp_compiler", "botbuild/app_schema")
    client = _anthropic_client("some response")
    langfuse_context.configure(enabled=False)
    feature_flags.refine_initial_prompt = True
    feature_flags.perplexity = True
    
    with tempfile.TemporaryDirectory() as tempdir:
        application = Application(client, compiler)
        my_bot = application.create_bot("Create a bot that does something please")

        interpolator = Interpolator(os.path.join(os.path.dirname(os.path.abspath(__file__)), ".."))
        interpolator.bake(my_bot, tempdir)

        #assert client.messages.create.call_count == 6
        assert my_bot.refined_description is not None
        assert my_bot.typespec.error_output is None
        assert my_bot.gherkin is not None
        assert my_bot.typescript_schema.error_output is None
        assert my_bot.drizzle.error_output is None

        for x in my_bot.handlers.values():
            assert x.error_output is None

        print("Generation complete, testing in docker")
        # change directory to tempdir and run docker compose
        current_dir = os.getcwd()
        os.chdir(tempdir)

        def generate_random_name(prefix, length=8):
            return prefix + ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))

        env = os.environ.copy()
        env["APP_CONTAINER_NAME"] = generate_random_name("app_")
        env["POSTGRES_CONTAINER_NAME"] = generate_random_name("db_")
        env["NETWORK_NAME"] = generate_random_name("network_")
        env["RUN_MODE"] = "http-server"
        try:
            # Set a consistent project name to avoid using directory name as prefix
            cmd = ["docker", "compose", "-p", "botbuild", "up", "-d"]
            result = subprocess.run(cmd, check=False, env=env, capture_output=True, text=True)
            assert result.returncode == 0, f"Docker compose failed with error: {result.stderr}"
            time.sleep(5)
            client = docker.from_env()
            app_container = client.containers.get(env["APP_CONTAINER_NAME"])
            db_container = client.containers.get(env["POSTGRES_CONTAINER_NAME"])

            assert app_container.status == "running", f"App container {env['APP_CONTAINER_NAME']} is not running"
            assert db_container.status == "running", f"Postgres container {env['POSTGRES_CONTAINER_NAME']} is not running"

            aws_check = subprocess.run(
                ["aws", "sts", "get-caller-identity", "--profile", "dev"],
                capture_output=True,
                text=True
            )            
            aws_available = (aws_check.returncode == 0 and "UserId" in aws_check.stdout) or \
                           (os.environ.get("AWS_ACCESS_KEY_ID", "").strip() != "")        
            print("AWS is available, making a request to the http server")
            # only checking if aws is available if it is, so we have access to bedrock
            # make a request to the http server
            base_url = "http://localhost:8989"
            time.sleep(5)  # to ensure migrations are done
            # retry a few times to handle potential timeouts on slower machines
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = httpx.post(f"{base_url}/chat", json={"message": "hello", "user_id": "123"}, timeout=15)
                    break
                except httpx.HTTPError:
                    if attempt < max_retries - 1:
                        print(f"request timed out, retrying ({attempt+1}/{max_retries})")
                        time.sleep(3 * (attempt + 1))
                    else:
                        raise

            if aws_available:
                assert response.status_code == 200, f"Expected status code 200, got {response.status_code}"
                assert response.json()["reply"]
            else:
                assert response.status_code == 500, f"Expected status code 500, as AWS creds are not available and bot should fail"

        finally:
            try:
                cmd = ["docker", "compose", "-p", "botbuild", "down"]
                subprocess.run(cmd, check=True, env=env, capture_output=True, text=True)
            except subprocess.CalledProcessError as e:
                print(f"Error downing docker compose: {e}")
                raise e
            os.chdir(current_dir)
